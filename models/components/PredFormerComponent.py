import torch
from einops import rearrange
from einops.layers.torch import Rearrange
from timm.layers import DropPath, to_2tuple, trunc_normal_
from torch import einsum, nn
from utils.ConfigType import PredFormerComponentConfig


def sinusoidal_embedding(n_channels, dim):
    pe = torch.FloatTensor(
        [
            [p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]
            for p in range(n_channels)
        ]
    )
    pe[:, 0::2] = torch.sin(pe[:, 0::2])
    pe[:, 1::2] = torch.cos(pe[:, 1::2])
    return rearrange(pe, "... -> 1 ...")


class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)


class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)


class Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):
        super().__init__()
        inner_dim = dim_head * heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head**-0.5

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        self.to_out = (
            nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))
            if project_out
            else nn.Identity()
        )

    def forward(self, x):
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), qkv)
        dots = einsum("b h i d, b h j d -> b h i j", q, k) * self.scale

        attn = dots.softmax(dim=-1)

        out = einsum("b h i j, b h j d -> b h i d", attn, v)
        out = rearrange(out, "b h n d -> b n (h d)")
        out = self.to_out(out)
        return out


class SwiGLU(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.SiLU,
        norm_layer=None,
        bias=True,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)

        self.fc1_g = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0])
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = (
            norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        )
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])

    def init_weights(self):
        nn.init.ones_(self.fc1_g.bias)
        nn.init.normal_(self.fc1_g.weight, std=1e-6)

    def forward(self, x):
        x_gate = self.fc1_g(x)
        x = self.fc1_x(x)
        x = self.act(x_gate) * x
        x = self.drop1(x)
        x = self.norm(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x


class GatedTransformer(nn.Module):
    def __init__(
        self,
        dim,
        depth,
        heads,
        dim_head,
        mlp_dim,
        dropout=0.0,
        attn_dropout=0.0,
        drop_path=0.1,
    ):
        super().__init__()
        self.layers = nn.ModuleList([])
        self.norm = nn.LayerNorm(dim)
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        PreNorm(
                            dim,
                            Attention(
                                dim,
                                heads=heads,
                                dim_head=dim_head,
                                dropout=attn_dropout,
                            ),
                        ),
                        PreNorm(dim, SwiGLU(dim, mlp_dim, drop=dropout)),
                        DropPath(drop_path) if drop_path > 0.0 else nn.Identity(),
                        DropPath(drop_path) if drop_path > 0.0 else nn.Identity(),
                    ]
                )
            )
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        for attn, ff, drop_path1, drop_path2 in self.layers:
            x = x + drop_path1(attn(x))
            x = x + drop_path2(ff(x))
        return self.norm(x)


class PredFormerLayer(nn.Module):
    def __init__(
        self,
        dim,
        depth,
        heads,
        dim_head,
        mlp_dim,
        dropout=0.0,
        attn_dropout=0.0,
        drop_path=0.1,
    ):
        super(PredFormerLayer, self).__init__()

        self.ts_temporal_transformer = GatedTransformer(
            dim, depth, heads, dim_head, mlp_dim, dropout, attn_dropout, drop_path
        )
        self.ts_space_transformer = GatedTransformer(
            dim, depth, heads, dim_head, mlp_dim, dropout, attn_dropout, drop_path
        )
        self.st_space_transformer = GatedTransformer(
            dim, depth, heads, dim_head, mlp_dim, dropout, attn_dropout, drop_path
        )
        self.st_temporal_transformer = GatedTransformer(
            dim, depth, heads, dim_head, mlp_dim, dropout, attn_dropout, drop_path
        )

    def forward(self, x):
        b, t, n, _ = x.shape
        x_ts, x_ori = x, x

        # ts-t branch
        x_ts = rearrange(x_ts, "b t n d -> b n t d")
        x_ts = rearrange(x_ts, "b n t d -> (b n) t d")
        x_ts = self.ts_temporal_transformer(x_ts)

        # ts-s branch
        x_ts = rearrange(x_ts, "(b n) t d -> b n t d", b=b)
        x_ts = rearrange(x_ts, "b n t d -> b t n d")
        x_ts = rearrange(x_ts, "b t n d -> (b t) n d")
        x_ts = self.ts_space_transformer(x_ts)

        # ts output branch
        x_ts = rearrange(x_ts, "(b t) n d -> b t n d", b=b)

        # add
        # x_ts += x_ori

        x_st, x_ori = x_ts, x_ts

        # st-s branch
        x_st = rearrange(x_st, "b t n d -> (b t) n d")
        x_st = self.st_space_transformer(x_st)

        # st-t branch
        x_st = rearrange(x_st, "(b t) ... -> b t ...", b=b)
        x_st = x_st.permute(0, 2, 1, 3)  # b n T d
        x_st = rearrange(x_st, "b n t d -> (b n) t d")
        x_st = self.st_temporal_transformer(x_st)

        # st output branch
        x_st = rearrange(x_st, "(b n) t d -> b n t d", b=b)
        x_st = rearrange(x_st, "b n t d -> b t n d", b=b)

        return x_st


class PredFormer_Model(nn.Module):
    def __init__(self, config: PredFormerComponentConfig):
        super().__init__()
        self.image_height = config.height
        self.image_width = config.width
        self.patch_size = config.patch_size
        self.num_patches_h = self.image_height // self.patch_size
        self.num_patches_w = self.image_width // self.patch_size
        self.num_patches = self.num_patches_h * self.num_patches_w

        # Model parameters
        self.num_frames_in = config.pre_seq
        self.dim = config.dim
        self.num_channels = config.num_channels
        self.num_classes = self.num_channels

        assert self.image_height % self.patch_size == 0, (
            "Image height must be divisible by the patch size."
        )
        assert self.image_width % self.patch_size == 0, (
            "Image width must be divisible by the patch size."
        )
        self.patch_dim = self.num_channels * self.patch_size**2
        self.to_patch_embedding = nn.Sequential(
            Rearrange(
                "b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)",
                p1=self.patch_size,
                p2=self.patch_size,
            ),
            nn.Linear(self.patch_dim, self.dim),
        )
        self.pos_embedding = nn.Parameter(
            sinusoidal_embedding(self.num_frames_in * self.num_patches, self.dim),
            requires_grad=False,
        ).view(1, self.num_frames_in, self.num_patches, self.dim)

        self.blocks = nn.ModuleList(
            [
                PredFormerLayer(
                    dim=config.dim,
                    depth=config.depth,
                    heads=config.heads,
                    dim_head=config.dim_head,
                    mlp_dim=config.dim * config.scale_dim,
                    dropout=config.dropout,
                    attn_dropout=config.attn_dropout,
                    drop_path=config.drop_path,
                )
                for i in range(config.nDepth)
            ]
        )

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(self.dim),
            nn.Linear(self.dim, self.num_channels * self.patch_size**2),
        )

    def forward(self, x):
        B, T, C, H, W = x.shape

        # Patch Embedding
        x = self.to_patch_embedding(x)

        # Posion Embedding
        x += self.pos_embedding.to(x.device)

        # PredFormer Encoder
        for blk in self.blocks:
            x = blk(x)

        # MLP head
        x = self.mlp_head(x.reshape(-1, self.dim))
        x = x.view(
            B,
            T,
            self.num_patches_h,
            self.num_patches_w,
            C,
            self.patch_size,
            self.patch_size,
        )
        x = x.permute(0, 1, 4, 2, 5, 3, 6).reshape(B, T, C, H, W)

        return x
